{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNKyaE0tGErDpTDio0Ae+dB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IVornehm/Thesis/blob/main/Embeddings_and_Neural_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word Embeddings"
      ],
      "metadata": {
        "id": "QGCIN93h18br"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ej8uMcvoso-5"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os, re, ast, json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from collections import Counter, defaultdict\n",
        "from scipy.spatial.distance import pdist\n",
        "from scipy.cluster.hierarchy import linkage, fcluster\n",
        "import spacy\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from multiprocessing import cpu_count\n",
        "import torch\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from kneed import KneeLocator\n",
        "import matplotlib.pyplot as plt\n",
        "import ast\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "root_dir = \"/content/drive/My Drive/Thesis Linguistics\"\n",
        "file_path = \"/content/childes_data_filtered_2.xlsx\"\n",
        "output_path = os.path.join(root_dir, \"childes_data_filtered_2.xlsx\")\n",
        "\n",
        "def parse_lists(cell):\n",
        "    if isinstance(cell, str):\n",
        "        if \"|\" in cell:\n",
        "            return cell.split(\"|\")\n",
        "        try:\n",
        "            return ast.literal_eval(cell)\n",
        "        except:\n",
        "            return [cell]\n",
        "    return cell\n",
        "\n",
        "def extract_clean_tokens(utt):\n",
        "    raw_tokens = re.findall(r'\\b\\w+\\b', utt)\n",
        "    filtered_tokens = []\n",
        "    for raw_token in raw_tokens:\n",
        "        parts = raw_token.split(\"_\") if \"_\" in raw_token else [raw_token]\n",
        "        for token in parts:\n",
        "            t_lower = token.lower()\n",
        "            if not t_lower: continue\n",
        "            if any(sym in t_lower for sym in {\"&\", \"@\", \"=\", \"{\", \"}\"}): continue\n",
        "            if any(sub in t_lower for sub in {\"babbl\", \"moan\", \"vocal\"}): continue\n",
        "            if any(char.isdigit() for char in t_lower): continue\n",
        "            if t_lower in {\"xxx\", \"yyy\", \"www\", \"nonspeech\"}: continue\n",
        "            if len(t_lower) == 1 and t_lower not in {\"i\", \"a\", \"s\"}: continue\n",
        "            if len(t_lower) == 3 and t_lower[0] == t_lower[1] == t_lower[2]: continue\n",
        "            if re.search(r\"[aeiou]h$\", t_lower): continue\n",
        "            if t_lower in {\"haha\", \"uhhum\", \"uhhuhhaw\", \"mm\", \"er\", \"ew\", \"ha\", \"hee\"}: continue\n",
        "            if \"hm\" in t_lower or \"mh\" in t_lower or \"hrm\" in t_lower: continue\n",
        "            if re.search(r\"(.{2,}).*\\1\", t_lower): continue\n",
        "            filtered_tokens.append(t_lower)\n",
        "    return filtered_tokens\n",
        "\n",
        "def normalize_batch(vectors):\n",
        "    norms = np.linalg.norm(vectors, axis=1, keepdims=True)\n",
        "    return vectors / np.clip(norms, 1e-10, None)\n",
        "\n",
        "def compute_inequality(counter):\n",
        "    values = list(counter.values())\n",
        "    total = sum(values)\n",
        "    if total == 0: return [0.0] * 5\n",
        "    mean = np.mean(values)\n",
        "    std = np.std(values)\n",
        "    norm_cv = round(std / mean, 4) if mean > 0 else 0.0\n",
        "    gini = lambda x: (np.sum((2 * np.arange(1, len(x)+1) - len(x) - 1) * np.sort(x))) / (len(x) * np.sum(x))\n",
        "    gini_score = round(gini(values), 4)\n",
        "    top = sorted(values, reverse=True)\n",
        "    return [norm_cv, gini_score, round(top[0]/total,4), round(sum(top[:3])/total,4), round(sum(top[:5])/total,4)]\n",
        "\n",
        "df = pd.read_excel(file_path)\n",
        "df[\"RelativePath\"] = df[\"RelativePath\"].apply(parse_lists)\n",
        "\n",
        "file_data, all_utt_set = {}, set()\n",
        "all_paths = set(p for paths in df[\"RelativePath\"] for p in paths)\n",
        "for path in tqdm(all_paths, desc=\"Loading .cha files\"):\n",
        "    full = os.path.join(root_dir, path.replace(\"\\\\\", \"/\"))\n",
        "    if os.path.exists(full):\n",
        "        try:\n",
        "            with open(full, \"r\", encoding=\"utf-8\") as f:\n",
        "                file_data[path] = re.findall(r\"^(\\*[A-Z]+[A-Za-z0-9_]*):\\s*(.*)\", f.read(), flags=re.MULTILINE)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "\n",
        "\n",
        "speaker_tokens, speaker_utterances = {}, {}\n",
        "combined_vocab = set()\n",
        "all_utt_set = set()\n",
        "\n",
        "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Collecting utterances\"):\n",
        "    tokens, utts = {\"CHI\": [], \"MOT\": []}, {\"CHI\": [], \"MOT\": []}\n",
        "    for path in row[\"RelativePath\"]:\n",
        "        for spk_code, utt in file_data.get(path.replace(\"\\\\\", \"/\"), []):\n",
        "            spk = spk_code.lstrip(\"*\")\n",
        "            if spk in tokens:\n",
        "                segments = re.split(r\"[.?!]+\", utt)\n",
        "                for seg in segments:\n",
        "                    seg = seg.strip()\n",
        "                    if not seg:\n",
        "                        continue\n",
        "                    tks = extract_clean_tokens(seg)\n",
        "                    if tks:\n",
        "                        utt_str = \" \".join(tks)\n",
        "                        tokens[spk].append(tks)\n",
        "                        utts[spk].append(utt_str)\n",
        "                        combined_vocab.update(tks)\n",
        "                        all_utt_set.add(utt_str)\n",
        "    speaker_tokens[idx] = tokens\n",
        "    speaker_utterances[idx] = utts\n",
        "\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
        "word_pos_cache = {}\n",
        "for doc in tqdm(nlp.pipe(list(combined_vocab), batch_size=1000), total=len(combined_vocab)//1000+1, desc=\"POS tagging\"):\n",
        "    for token in doc:\n",
        "        if token.is_alpha:\n",
        "            word_pos_cache[token.text] = token.pos_ in {\"NOUN\", \"VERB\", \"ADJ\", \"ADV\"}\n",
        "content_word_types = {w for w, is_content in word_pos_cache.items() if is_content}\n",
        "\n",
        "\n",
        "for idx in speaker_tokens:\n",
        "    for spk in [\"CHI\", \"MOT\"]:\n",
        "        new_tokens, new_utts = [], []\n",
        "        for tks, utt in zip(speaker_tokens[idx][spk], speaker_utterances[idx][spk]):\n",
        "            if any(tok in content_word_types for tok in tks):\n",
        "                new_tokens.extend(tks)\n",
        "                new_utts.append(utt)\n",
        "        speaker_tokens[idx][spk] = new_tokens\n",
        "        speaker_utterances[idx][spk] = new_utts\n",
        "\n",
        "\n",
        "\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "word_emb_cache = dict(zip(list(combined_vocab), model.encode(list(combined_vocab), batch_size=256, show_progress_bar=True)))\n",
        "utt_emb_cache = dict(zip(list(all_utt_set), model.encode(list(all_utt_set), batch_size=256, show_progress_bar=True)))\n",
        "\n",
        "thresholds = [0.25, 0.50, 0.75, 1.00]\n",
        "metric_names = [\"TokenNormCV\", \"Gini\", \"MaxDominance\", \"Top3Dominance\", \"Top5Dominance\"]\n",
        "for spk in [\"CHI\", \"MOT\"]:\n",
        "    for t in thresholds:\n",
        "        df[f\"{spk}_EmbWord_t{t:.2f}\"] = np.nan\n",
        "        df[f\"{spk}_EmbUtt_t{t:.2f}\"] = np.nan\n",
        "        for m in metric_names:\n",
        "            df[f\"{spk}_{m}_EmbWord_t{t:.2f}\"] = np.nan\n",
        "            df[f\"{spk}_{m}_EmbUtt_t{t:.2f}\"] = np.nan\n",
        "\n",
        "centroid_records_word, centroid_records_utt = [], []\n",
        "\n",
        "def process_row(idx):\n",
        "    row_res = {}\n",
        "    for spk in [\"CHI\", \"MOT\"]:\n",
        "        words = [w for w in speaker_tokens[idx][spk] if w in content_word_types]\n",
        "        type_counts = Counter(words)\n",
        "        if len(type_counts) >= 2:\n",
        "            embs = normalize_batch(np.array([word_emb_cache[w] for w in type_counts]))\n",
        "            Z = linkage(pdist(embs, \"cosine\"), method=\"average\")\n",
        "            for t in thresholds:\n",
        "                clusters = fcluster(Z, t=t, criterion=\"distance\")\n",
        "                cluster_map = defaultdict(list)\n",
        "                for w, c in zip(type_counts.keys(), clusters):\n",
        "                    cluster_map[c].append((w, type_counts[w]))\n",
        "                row_res[f\"{spk}_EmbWord_t{t:.2f}\"] = len(cluster_map)\n",
        "                row_res.update({f\"{spk}_{m}_EmbWord_t{t:.2f}\": v for m, v in zip(metric_names, compute_inequality(Counter({c: sum(freq for _, freq in ws) for c, ws in cluster_map.items()})))})\n",
        "                for ws in cluster_map.values():\n",
        "                    emb = np.mean([word_emb_cache[w] for w, _ in ws], axis=0)\n",
        "                    centroid_records_word.append({\"RowID\": idx, \"Speaker\": spk, \"Threshold\": t, \"Centroid\": emb})\n",
        "\n",
        "        utts = [u for u in speaker_utterances[idx][spk] if u in utt_emb_cache]\n",
        "        if len(utts) >= 2:\n",
        "            embs = normalize_batch(np.array([utt_emb_cache[u] for u in utts]))\n",
        "            Z = linkage(pdist(embs, \"cosine\"), method=\"average\")\n",
        "            for t in thresholds:\n",
        "                clusters = fcluster(Z, t=t, criterion=\"distance\")\n",
        "                cluster_map = defaultdict(list)\n",
        "                for u, c in zip(utts, clusters):\n",
        "                    cluster_map[c].append(u)\n",
        "                row_res[f\"{spk}_EmbUtt_t{t:.2f}\"] = len(cluster_map)\n",
        "                row_res.update({f\"{spk}_{m}_EmbUtt_t{t:.2f}\": v for m, v in zip(metric_names, compute_inequality(Counter({c: len(us) for c, us in cluster_map.items()})))})\n",
        "                for us in cluster_map.values():\n",
        "                    emb = np.mean([utt_emb_cache[u] for u in us], axis=0)\n",
        "                    centroid_records_utt.append({\"RowID\": idx, \"Speaker\": spk, \"Threshold\": t, \"Centroid\": emb})\n",
        "    return idx, row_res\n",
        "\n",
        "print(\"Starting threaded row processing...\")\n",
        "with ThreadPoolExecutor(max_workers=min(16, cpu_count())) as executor:\n",
        "    futures = [executor.submit(process_row, i) for i in df.index]\n",
        "    for future in tqdm(as_completed(futures), total=len(futures), desc=\"Row clustering\"):\n",
        "        idx, res = future.result()\n",
        "        for k, v in res.items():\n",
        "            df.at[idx, k] = v\n",
        "\n",
        "\n",
        "\n",
        "def assign_meta_topics_and_save(centroids, label_prefix, emb_cache, df, t, save_dir):\n",
        "    print(f\"Clustering {len(centroids)} centroids for {label_prefix} at threshold t={t:.2f}\")\n",
        "\n",
        "    mat = np.vstack([c[\"Centroid\"] for c in centroids])\n",
        "    mat = normalize_batch(mat)\n",
        "\n",
        "\n",
        "    distortions = []\n",
        "    trial_ks = list(range(10, 301, 10))\n",
        "    for k in trial_ks:\n",
        "        kmeans = MiniBatchKMeans(n_clusters=k, random_state=42, batch_size=128, n_init=\"auto\").fit(mat)\n",
        "        distortions.append(kmeans.inertia_)\n",
        "\n",
        "\n",
        "    def smooth(values, window=3):\n",
        "        return [np.mean(values[max(0, i-window+1):i+1]) for i in range(len(values))]\n",
        "\n",
        "    smooth_distortions = smooth(distortions)\n",
        "    kneedle = KneeLocator(trial_ks, smooth_distortions, curve=\"convex\", direction=\"decreasing\")\n",
        "\n",
        "\n",
        "    if kneedle.elbow:\n",
        "        k = kneedle.elbow\n",
        "        inertia = distortions[trial_ks.index(k)]\n",
        "        print(f\"Elbow found (smoothed): k={k}, inertia={inertia:.2f} for {label_prefix} at t={t:.2f}\")\n",
        "\n",
        "    else:\n",
        "        print(f\"No elbow found after smoothing. Trying silhouette fallback for {label_prefix} at t={t:.2f}\")\n",
        "\n",
        "\n",
        "        print(f\"No elbow found after smoothing. Trying silhouette fallback \"\n",
        "              f\"for {label_prefix} at t={t:.2f}\")\n",
        "\n",
        "        max_centroids = 10000\n",
        "\n",
        "        if len(mat) > max_centroids:\n",
        "            print(f\"Subsampling {max_centroids} of {len(mat)} centroids \"\n",
        "                  \"for silhouette selection\")\n",
        "            sample_idx   = np.random.choice(len(mat), max_centroids, replace=False)\n",
        "            mat_sample   = mat[sample_idx]\n",
        "        else:\n",
        "            mat_sample   = mat\n",
        "\n",
        "        best_k, best_score = None, -1\n",
        "        for k_try in trial_ks:\n",
        "            if k_try >= len(mat_sample):\n",
        "                continue\n",
        "            kmeans  = MiniBatchKMeans(\n",
        "                        n_clusters=k_try,\n",
        "                        random_state=42,\n",
        "                        batch_size=128,\n",
        "                        n_init=\"auto\"\n",
        "                    ).fit(mat_sample)\n",
        "            score   = silhouette_score(mat_sample, kmeans.labels_)\n",
        "            if score > best_score:\n",
        "                best_k, best_score = k_try, score\n",
        "\n",
        "        k = best_k\n",
        "        print(f\"Using silhouette fallback: k={k} \"\n",
        "              f\"(silhouette = {best_score:.4f}) for {label_prefix} at t={t:.2f}\")\n",
        "\n",
        "    method_used = \"elbow\" if kneedle.elbow else \"silhouette\"\n",
        "    print(f\"Final k={k} used via {method_used} for {label_prefix} at t={t:.2f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    kmeans = MiniBatchKMeans(n_clusters=k, random_state=42, batch_size=128, n_init=\"auto\", verbose=0).fit(mat)\n",
        "    labels = kmeans.labels_\n",
        "\n",
        "\n",
        "    topic_counts = defaultdict(lambda: {\"CHI\": [0]*k, \"MOT\": [0]*k})\n",
        "    for rec, topic_id in zip(centroids, labels):\n",
        "        row_id = rec[\"RowID\"]\n",
        "        speaker = rec[\"Speaker\"]\n",
        "        topic_counts[row_id][speaker][topic_id] += 1\n",
        "\n",
        "    for topic_id in range(k):\n",
        "        for spk in [\"CHI\", \"MOT\"]:\n",
        "            colname = f\"{label_prefix}_{spk}_MetaTopic_{topic_id:02d}_t{t:.2f}\"\n",
        "            df[colname] = df.index.map(\n",
        "                lambda i: topic_counts[i][spk][topic_id] if i in topic_counts and spk in topic_counts[i] else 0\n",
        "            )\n",
        "\n",
        "    all_items = list(emb_cache.keys())\n",
        "    all_embs = normalize_batch(np.vstack([emb_cache[i] for i in all_items]))\n",
        "    output = {}\n",
        "\n",
        "    nlp_single = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    item_speaker_map = {}\n",
        "\n",
        "    for idx in df.index:\n",
        "        if label_prefix == \"Word\":\n",
        "            for word in speaker_tokens[idx][\"MOT\"]:\n",
        "                item_speaker_map[word] = \"MOT\"\n",
        "            for word in speaker_tokens[idx][\"CHI\"]:\n",
        "                item_speaker_map[word] = \"CHI\"\n",
        "\n",
        "\n",
        "        elif label_prefix == \"Utt\":\n",
        "            for utt in speaker_utterances[idx][\"MOT\"]:\n",
        "                item_speaker_map[utt] = \"MOT\"\n",
        "            for utt in speaker_utterances[idx][\"CHI\"]:\n",
        "                item_speaker_map[utt] = \"CHI\"\n",
        "\n",
        "\n",
        "\n",
        "    for topic_id in range(k):\n",
        "        centroid = kmeans.cluster_centers_[topic_id]\n",
        "        dists = np.linalg.norm(all_embs - centroid, axis=1)\n",
        "        sorted_indices = np.argsort(dists)\n",
        "\n",
        "        top5 = []\n",
        "\n",
        "\n",
        "        if label_prefix == \"Word\":\n",
        "\n",
        "\n",
        "            dists_topic = dists[sorted_indices]\n",
        "            words_topic = [all_items[i] for i in sorted_indices]\n",
        "\n",
        "            percentiles = [0, 5, 10, 15, 20, 25]\n",
        "\n",
        "            cutoffs = np.percentile(dists_topic, percentiles)\n",
        "\n",
        "            selected_words = []\n",
        "            used_indices = set()\n",
        "\n",
        "            for pct_val, pct_label in zip(cutoffs, percentiles):\n",
        "                idx = np.argmin(np.abs(dists_topic - pct_val))\n",
        "                if idx in used_indices:\n",
        "                    continue\n",
        "                used_indices.add(idx)\n",
        "                word = words_topic[idx]\n",
        "                selected_words.append({\n",
        "                    \"word\": word,\n",
        "                    \"speaker\": item_speaker_map.get(word, \"UNKNOWN\"),\n",
        "                    \"distance\": round(float(dists_topic[idx]), 4),\n",
        "                    \"percentile\": pct_label\n",
        "                })\n",
        "\n",
        "            output[f\"{label_prefix}_MetaTopic_{topic_id:02d}_t{t:.2f}\"] = selected_words\n",
        "\n",
        "        else:\n",
        "            dists_topic = dists[sorted_indices]\n",
        "            utts_topic = [all_items[i] for i in sorted_indices]\n",
        "\n",
        "            percentiles = [0, 5, 10, 15, 20, 25]\n",
        "\n",
        "            cutoffs = np.percentile(dists_topic, percentiles)\n",
        "\n",
        "            selected_utts = []\n",
        "            used_indices = set()\n",
        "\n",
        "            for pct_val, pct_label in zip(cutoffs, percentiles):\n",
        "                idx = np.argmin(np.abs(dists_topic - pct_val))\n",
        "                if idx in used_indices:\n",
        "                    continue\n",
        "                used_indices.add(idx)\n",
        "\n",
        "                selected_utts.append({\n",
        "                    \"utt\": utts_topic[idx],\n",
        "                    \"speaker\": item_speaker_map.get(utts_topic[idx], \"UNKNOWN\"),\n",
        "                    \"distance\": round(float(dists_topic[idx]), 4),\n",
        "                    \"percentile\": pct_label\n",
        "                })\n",
        "\n",
        "            output[f\"{label_prefix}_MetaTopic_{topic_id:02d}_t{t:.2f}\"] = selected_utts\n",
        "\n",
        "    out_path = os.path.join(save_dir, f\"{label_prefix}_MetaTopic_TopItems_t{t:.2f}.json\")\n",
        "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(output, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "\n",
        "for t in thresholds:\n",
        "    if t < 1.00:\n",
        "        word_centroids = [c for c in centroid_records_word if c[\"Threshold\"] == t]\n",
        "        assign_meta_topics_and_save(word_centroids, \"Word\", word_emb_cache, df, t, root_dir)\n",
        "\n",
        "        utt_centroids = [c for c in centroid_records_utt if c[\"Threshold\"] == t]\n",
        "        assign_meta_topics_and_save(utt_centroids, \"Utt\", utt_emb_cache, df, t, root_dir)\n",
        "\n",
        "\n",
        "df.to_excel(output_path, index=False)\n",
        "print(\"Meta-topic distributions and top items saved.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Anova"
      ],
      "metadata": {
        "id": "2yMgJNDe1iLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import statsmodels.formula.api as smf\n",
        "from statsmodels.stats.anova import anova_lm\n",
        "\n",
        "df = pd.read_excel(\"/content/childes_data_filtered_2.xlsx\")\n",
        "\n",
        "suffixes = [\n",
        "    \"Raw\", \"Content\", \"ContentLemma\", \"ContentLemmaPOS\", \"Lex\", \"Syn\",\n",
        "    \"EmbWord_t0.25\", \"EmbWord_t0.50\", \"EmbWord_t0.75\"\n",
        "]\n",
        "\n",
        "outcome_labels = {\n",
        "    \"CHI_TTR\": \"TTR\",\n",
        "    \"CHI_CV\": \"CV\",\n",
        "    \"CHI_Gini\": \"Gini\",\n",
        "    \"CHI_Top3Dominance\": \"Top-3\"\n",
        "}\n",
        "\n",
        "suffix_labels = {\n",
        "    \"Raw\": \"All Words\",\n",
        "    \"Content\": \"Content Words\",\n",
        "    \"ContentLemma\": \"Lemmatized Content Words\",\n",
        "    \"ContentLemmaPOS\": \"Lemmatized Content Words with POS\",\n",
        "    \"Lex\": \"Lexical Category\",\n",
        "    \"Syn\": \"Synonyms\",\n",
        "    \"EmbWord_t0.25\": \"Word Embeddings (T=0.25)\",\n",
        "    \"EmbWord_t0.50\": \"Word Embeddings (T=0.50)\",\n",
        "    \"EmbWord_t0.75\": \"Word Embeddings (T=0.75)\"\n",
        "}\n",
        "\n",
        "rows = []\n",
        "\n",
        "for suffix in suffixes:\n",
        "    label = suffix_labels.get(suffix, suffix)\n",
        "    rows.append({\n",
        "        \"Suffix\": label,\n",
        "        \"Outcome\": \"\",\n",
        "        \"F_Study\": \"\", \"p_Study\": \"\", \"F_Child\": \"\", \"p_Child\": \"\"\n",
        "    })\n",
        "\n",
        "    for prefix, outcome_name in outcome_labels.items():\n",
        "        outcome = f\"{prefix}_{suffix}\"\n",
        "        if outcome not in df.columns:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            model_study = smf.ols(f\"Q('{outcome}') ~ C(Study)\", data=df).fit()\n",
        "            anova_study = anova_lm(model_study)\n",
        "            f_study = anova_study.loc[\"C(Study)\", \"F\"]\n",
        "            p_study = anova_study.loc[\"C(Study)\", \"PR(>F)\"]\n",
        "            p_study_fmt = r\"\\textless{}0.001\" if p_study < 0.001 else f\"{p_study:.3f}\"\n",
        "\n",
        "\n",
        "            model_child = smf.ols(f\"Q('{outcome}') ~ C(NumericID)\", data=df).fit()\n",
        "            anova_child = anova_lm(model_child)\n",
        "            f_child = anova_child.loc[\"C(NumericID)\", \"F\"]\n",
        "            p_child = anova_child.loc[\"C(NumericID)\", \"PR(>F)\"]\n",
        "            p_child_fmt = r\"\\textless{}0.001\" if p_child < 0.001 else f\"{p_child:.3f}\"\n",
        "\n",
        "\n",
        "            rows.append({\n",
        "                \"Suffix\": \"\",\n",
        "                \"Outcome\": outcome_name,\n",
        "                \"F_Study\": f\"{f_study:.2f}\",\n",
        "                \"p_Study\": p_study_fmt,\n",
        "                \"F_Child\": f\"{f_child:.2f}\",\n",
        "                \"p_Child\": p_child_fmt\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error with {outcome}: {e}\")\n",
        "            continue\n",
        "\n",
        "table_df = pd.DataFrame(rows)\n",
        "\n",
        "latex = table_df.to_latex(\n",
        "    index=False,\n",
        "    escape=False,\n",
        "    caption=\"ANOVA results for effects of Study and Child on each outcome, grouped by input type.\",\n",
        "    label=\"tab:anova_longformat_descriptive\",\n",
        "    column_format=\"llcccc\",\n",
        "    header=[\"Input Type\", \"Outcome\", \"F (Study)\", \"p (Study)\", \"F (Child)\", \"p (Child)\"]\n",
        ")\n",
        "\n",
        "print(latex)\n"
      ],
      "metadata": {
        "id": "1JXnUF0ys-01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neural Network"
      ],
      "metadata": {
        "id": "S0UbILr51m0u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import joblib\n",
        "import random\n",
        "import os\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "file_path = \"/content/childes_data_filtered_2.xlsx\"\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "lang_encoder = LabelEncoder()\n",
        "study_encoder = LabelEncoder()\n",
        "child_encoder = LabelEncoder()\n",
        "gender_encoder = LabelEncoder()\n",
        "\n",
        "df[\"Language_encoded\"] = lang_encoder.fit_transform(df[\"Language\"].astype(str))\n",
        "df[\"Study_encoded\"] = study_encoder.fit_transform(df[\"Study\"].astype(str))\n",
        "df[\"Child_encoded\"] = child_encoder.fit_transform(df[\"NumericID\"].astype(str))\n",
        "df[\"Gender_encoded\"] = gender_encoder.fit_transform(df[\"Gender\"].astype(str))\n",
        "\n",
        "\n",
        "suffixes = [\n",
        "    \"Raw\", \"Content\", \"ContentLemma\", \"ContentLemmaPOS\", \"Lex\", \"Syn\",\n",
        "    \"EmbWord_t0.25\", \"EmbWord_t0.50\", \"EmbWord_t0.75\"\n",
        "]\n",
        "\n",
        "\n",
        "configs = []\n",
        "\n",
        "for suffix in suffixes:\n",
        "    configs.append({\n",
        "        \"input_suffix\": suffix,\n",
        "        \"output_suffix\": suffix,\n",
        "        \"model_name\": f\"best_model_ROTATED_{suffix}_TO_{suffix}.pt\"\n",
        "    })\n",
        "\n",
        "content_inputs = [\"Lex\", \"Syn\", \"EmbWord_t0.25\", \"EmbWord_t0.50\", \"EmbWord_t0.75\"]\n",
        "for suffix in content_inputs:\n",
        "    configs.append({\n",
        "        \"input_suffix\": suffix,\n",
        "        \"output_suffix\": \"Content\",\n",
        "        \"model_name\": f\"best_model_ROTATED_{suffix}_TO_Content.pt\"\n",
        "    })\n",
        "\n",
        "\n",
        "\n",
        "class TTRDataset(Dataset):\n",
        "    def __init__(self, df, feature_columns, target_cols):\n",
        "        self.features = torch.tensor(df[feature_columns].values, dtype=torch.float32)\n",
        "        self.language = torch.tensor(df[\"Language_encoded\"].values, dtype=torch.long)\n",
        "        self.study = torch.tensor(df[\"Study_encoded\"].values, dtype=torch.long)\n",
        "        self.child = torch.tensor(df[\"Child_encoded\"].values, dtype=torch.long)\n",
        "        self.gender = torch.tensor(df[\"Gender_encoded\"].values, dtype=torch.long)\n",
        "        self.targets = torch.tensor(df[target_cols].values, dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.targets)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            \"features\": self.features[idx],\n",
        "            \"language\": self.language[idx],\n",
        "            \"study\": self.study[idx],\n",
        "            \"child\": self.child[idx],\n",
        "            \"gender\": self.gender[idx],\n",
        "            \"target\": self.targets[idx],\n",
        "        }\n",
        "\n",
        "class MultitaskTTRModel(nn.Module):\n",
        "    def __init__(self, n_langs, n_studies, n_children, n_genders, input_dim, embed_dim=8):\n",
        "        super().__init__()\n",
        "        self.lang_embed = nn.Embedding(n_langs, embed_dim)\n",
        "        self.study_embed = nn.Embedding(n_studies, embed_dim)\n",
        "        self.child_embed = nn.Embedding(n_children, embed_dim)\n",
        "        self.gender_embed = nn.Embedding(n_genders, embed_dim)\n",
        "\n",
        "        self.shared = nn.Sequential(\n",
        "            nn.Linear(input_dim + 4 * embed_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.ttr = nn.Sequential(nn.Linear(32, 1), nn.Sigmoid())\n",
        "        self.cv = nn.Sequential(nn.Linear(32, 1), nn.ReLU())\n",
        "        self.gini = nn.Sequential(nn.Linear(32, 1), nn.Sigmoid())\n",
        "        self.dom = nn.Sequential(nn.Linear(32, 1), nn.Sigmoid())\n",
        "\n",
        "    def forward(self, features, language, study, child, gender):\n",
        "        x = torch.cat([\n",
        "            features,\n",
        "            self.lang_embed(language),\n",
        "            self.study_embed(study),\n",
        "            self.child_embed(child),\n",
        "            self.gender_embed(gender)\n",
        "        ], dim=1)\n",
        "        x = self.shared(x)\n",
        "        return torch.cat([self.ttr(x), self.cv(x), self.gini(x), self.dom(x)], dim=1)\n",
        "\n",
        "latex_results = []\n",
        "\n",
        "\n",
        "for cfg in configs:\n",
        "    input_suffix = cfg[\"input_suffix\"]\n",
        "    output_suffix = cfg[\"output_suffix\"]\n",
        "\n",
        "    print(f\"\\n=== Cross-validating: MOT input {input_suffix} ‚Üí CHI output {output_suffix} ===\")\n",
        "\n",
        "    feature_columns = [\n",
        "        f\"MOT_TTR_{input_suffix}\", f\"MOT_CV_{input_suffix}\",\n",
        "        f\"MOT_Gini_{input_suffix}\", f\"MOT_Top3Dominance_{input_suffix}\",\n",
        "        \"MOT_AvgWordLength\", \"Rel_MOT_Turns\", \"MOT_AvgTurnLength\",\n",
        "        \"MOT_Concreteness\", \"Age_in_Days\"\n",
        "    ]\n",
        "\n",
        "    target_cols = [\n",
        "        f\"CHI_TTR_{output_suffix}\", f\"CHI_CV_{output_suffix}\",\n",
        "        f\"CHI_Gini_{output_suffix}\", f\"CHI_Top3Dominance_{output_suffix}\"\n",
        "    ]\n",
        "\n",
        "    required_cols = [\"Language_encoded\", \"Study_encoded\", \"Child_encoded\", \"Gender_encoded\"] + feature_columns + target_cols\n",
        "    data = df.dropna(subset=required_cols).copy()\n",
        "    if data.empty:\n",
        "        print(f\"Skipping {input_suffix} ‚Üí {output_suffix} ‚Äî no data after dropna.\")\n",
        "        continue\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    data[feature_columns] = scaler.fit_transform(data[feature_columns])\n",
        "\n",
        "    all_preds, all_targets = [], []\n",
        "    gkf = GroupKFold(n_splits=5)\n",
        "\n",
        "\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(gkf.split(data, groups=data[\"Study_encoded\"])):\n",
        "        print(f\"Fold {fold + 1}/5\")\n",
        "        train_data = data.iloc[train_idx]\n",
        "        val_data = data.iloc[val_idx]\n",
        "\n",
        "        train_dataset = TTRDataset(train_data, feature_columns, target_cols)\n",
        "        val_dataset = TTRDataset(val_data, feature_columns, target_cols)\n",
        "        train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=64)\n",
        "\n",
        "        model = MultitaskTTRModel(\n",
        "            n_langs=len(lang_encoder.classes_),\n",
        "            n_studies=len(study_encoder.classes_),\n",
        "            n_children=len(child_encoder.classes_),\n",
        "            n_genders=len(gender_encoder.classes_),\n",
        "            input_dim=len(feature_columns)\n",
        "        ).to(device)\n",
        "\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "        loss_fn = nn.MSELoss()\n",
        "        best_val_loss = float(\"inf\")\n",
        "        patience = 3\n",
        "        patience_counter = 0\n",
        "\n",
        "        for epoch in range(1, 51):\n",
        "            model.train()\n",
        "            for batch in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                preds = model(batch[\"features\"].to(device), batch[\"language\"].to(device),\n",
        "                              batch[\"study\"].to(device), batch[\"child\"].to(device), batch[\"gender\"].to(device))\n",
        "                targets = batch[\"target\"].to(device)\n",
        "                loss = loss_fn(preds, targets)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            model.eval()\n",
        "            val_preds, val_targets = [], []\n",
        "            with torch.no_grad():\n",
        "                for batch in val_loader:\n",
        "                    preds = model(batch[\"features\"].to(device), batch[\"language\"].to(device),\n",
        "                                  batch[\"study\"].to(device), batch[\"child\"].to(device), batch[\"gender\"].to(device)).cpu().numpy()\n",
        "                    targets = batch[\"target\"].cpu().numpy()\n",
        "                    val_preds.append(preds)\n",
        "                    val_targets.append(targets)\n",
        "\n",
        "            val_preds = np.vstack(val_preds)\n",
        "            val_targets = np.vstack(val_targets)\n",
        "            val_loss = loss_fn(torch.tensor(val_preds), torch.tensor(val_targets)).item()\n",
        "\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                patience_counter = 0\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "                if patience_counter >= patience:\n",
        "                    break\n",
        "\n",
        "        all_preds.append(val_preds)\n",
        "        all_targets.append(val_targets)\n",
        "\n",
        "    all_preds = np.vstack(all_preds)\n",
        "    all_targets = np.vstack(all_targets)\n",
        "\n",
        "    metric_names = [\"TTR\", \"CV\", \"Gini\", \"Top3\"]\n",
        "    print(f\"\\n Evaluation for: MOT_{input_suffix} ‚Üí CHI_{output_suffix}\")\n",
        "    for i, name in enumerate(metric_names):\n",
        "        y_true = all_targets[:, i]\n",
        "        y_pred = all_preds[:, i]\n",
        "        rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "        mae = mean_absolute_error(y_true, y_pred)\n",
        "        r2 = r2_score(y_true, y_pred)\n",
        "        print(f\"{name:>5} ‚Äî RMSE: {rmse:.4f}, MAE: {mae:.4f}, R¬≤: {r2:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "        latex_results.append({\n",
        "            \"Input\": input_suffix,\n",
        "            \"Output\": output_suffix,\n",
        "            \"Metric\": name,\n",
        "            \"RMSE\": rmse,\n",
        "            \"MAE\": mae,\n",
        "            \"R2\": r2\n",
        "        })\n",
        "\n",
        "        plt.figure()\n",
        "        plt.scatter(y_true, y_pred, alpha=0.5, label=\"Predictions\")\n",
        "        plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', label=\"Ideal\")\n",
        "        plt.xlabel(f\"Actual {name}\")\n",
        "        plt.ylabel(f\"Predicted {name}\")\n",
        "        plt.title(f\"{input_suffix} ‚Üí {output_suffix} | {name}\")\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    print(f\"\\nüü¢ Final training on full data: {input_suffix} ‚Üí {output_suffix}\")\n",
        "    full_dataset = TTRDataset(data, feature_columns, target_cols)\n",
        "    full_loader = DataLoader(full_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "    model = MultitaskTTRModel(\n",
        "        n_langs=len(lang_encoder.classes_),\n",
        "        n_studies=len(study_encoder.classes_),\n",
        "        n_children=len(child_encoder.classes_),\n",
        "        n_genders=len(gender_encoder.classes_),\n",
        "        input_dim=len(feature_columns)\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    loss_fn = nn.MSELoss()\n",
        "    patience = 3\n",
        "    patience_counter = 0\n",
        "    best_loss = float(\"inf\")\n",
        "\n",
        "    for epoch in range(1, 51):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for batch in full_loader:\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(batch[\"features\"].to(device), batch[\"language\"].to(device),\n",
        "                          batch[\"study\"].to(device), batch[\"child\"].to(device), batch[\"gender\"].to(device))\n",
        "            targets = batch[\"target\"].to(device)\n",
        "            loss = loss_fn(preds, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(full_loader)\n",
        "        print(f\"Epoch {epoch:02} | Full Train Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        if avg_loss < best_loss:\n",
        "            best_loss = avg_loss\n",
        "            patience_counter = 0\n",
        "            torch.save(model.state_dict(), cfg[\"model_name\"])\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(\"‚èπÔ∏è Early stopping.\")\n",
        "                break\n",
        "\n",
        "\n",
        "\n",
        "suffix_code = {\n",
        "    \"Raw\": \"A\",\n",
        "    \"Content\": \"C\",\n",
        "    \"ContentLemma\": \"CL\",\n",
        "    \"ContentLemmaPOS\": \"CLP\",\n",
        "    \"Lex\": \"LEX\",\n",
        "    \"Syn\": \"SYN\",\n",
        "    \"EmbWord_t0.25\": \"WEMB (0.25)\",\n",
        "    \"EmbWord_t0.50\": \"WEMB (0.50)\",\n",
        "    \"EmbWord_t0.75\": \"WEMB (0.75)\",\n",
        "}\n",
        "\n",
        "\n",
        "results_df = pd.DataFrame(latex_results)\n",
        "\n",
        "for col in [\"RMSE\", \"MAE\", \"R2\"]:\n",
        "    results_df[col] = pd.to_numeric(results_df[col], errors=\"coerce\")\n",
        "\n",
        "\n",
        "results_df[\"Input\"] = results_df[\"Input\"].map(suffix_code)\n",
        "results_df[\"Output\"] = results_df[\"Output\"].map(suffix_code)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "table = results_df.pivot_table(\n",
        "    index=[\"Input\", \"Output\"],\n",
        "    columns=\"Metric\",\n",
        "    values=[\"RMSE\", \"MAE\", \"R2\"],\n",
        "    aggfunc=\"mean\"\n",
        ")\n",
        "\n",
        "\n",
        "table.columns = [f\"{stat}_{metric}\" for stat, metric in table.columns]\n",
        "table.reset_index(inplace=True)\n",
        "\n",
        "\n",
        "order = []\n",
        "for metric in [\"Gini\", \"CV\", \"TTR\", \"Top3\"]:\n",
        "    order.extend([f\"{stat}_{metric}\" for stat in [\"RMSE\", \"MAE\", \"R2\"]])\n",
        "table = table[[\"Input\", \"Output\"] + order]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "col_format = \"ll\" + \"ccc\" * 4\n",
        "column_labels = [\"Gini\", \"CV\", \"TTR\", \"Top-3 Proportion\"]\n",
        "sub_labels = [\"RMSE\", \"MAE\", \"R$^2$\"] * 4\n",
        "\n",
        "\n",
        "header1 = (\n",
        "    \" & \".join([\"Input\", \"Output\"] + [f\"\\\\multicolumn{{3}}{{c}}{{{label}}}\" for label in column_labels])\n",
        "    + \" \\\\\\\\\"\n",
        ")\n",
        "header2 = \" & \".join([\"\", \"\"] + sub_labels) + \" \\\\\\\\\"\n",
        "midrule = (\n",
        "    \"\\\\cmidrule(lr){3-5} \"\n",
        "    \"\\\\cmidrule(lr){6-8} \"\n",
        "    \"\\\\cmidrule(lr){9-11} \"\n",
        "    \"\\\\cmidrule(lr){12-14}\"\n",
        ")\n",
        "\n",
        "\n",
        "latex_body = table.to_latex(\n",
        "    index=False,\n",
        "    float_format=\"%.3f\",\n",
        "    column_format=col_format,\n",
        "    header=False,\n",
        "    escape=False\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "lines = latex_body.splitlines()\n",
        "lines.insert(3, header2)\n",
        "lines.insert(3, midrule)\n",
        "lines.insert(3, header1)\n",
        "\n",
        "\n",
        "lines[0] = \"\\\\begin{tabular}{\" + col_format + \"}\"\n",
        "lines[1] = \"\\\\toprule\"\n",
        "\n",
        "\n",
        "lines.append(\"\\\\bottomrule\")\n",
        "lines.append(\"\\\\end{tabular}\")\n",
        "\n",
        "\n",
        "latex_final = \"\\n\".join(lines)\n",
        "print(latex_final)\n"
      ],
      "metadata": {
        "id": "nVvu8ee21o5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simulations"
      ],
      "metadata": {
        "id": "0qdbAmZ81tDi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "import torch.nn as nn\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "from matplotlib.ticker import FormatStrFormatter, MaxNLocator\n",
        "import gc\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "df = pd.read_excel(\"/content/childes_data_filtered_2.xlsx\")\n",
        "df[\"Age_in_Days\"] = df[\"Age_in_Days\"].astype(float)\n",
        "\n",
        "suffix_labels = {\n",
        "    \"Raw\": \"All Words\",\n",
        "    \"Content\": \"Content Words\",\n",
        "    \"ContentLemma\": \"Lemmatized Content Words\",\n",
        "    \"ContentLemmaPOS\": \"Lemmatized Content Words with POS\",\n",
        "    \"Syn\": \"First Synset Identifier\",\n",
        "    \"Lex\": \"Lexical Category\",\n",
        "    \"EmbWord_t0.25\": \"Word Embeddings  T = 0.25\",\n",
        "    \"EmbWord_t0.50\": \"Word Embeddings T = 0.5\",\n",
        "    \"EmbWord_t0.75\": \"Word Embeddings T = 0.75\",\n",
        "}\n",
        "\n",
        "var_labels = {\n",
        "    \"TTR\": \"TTR\",\n",
        "    \"CV\": \"CV\",\n",
        "    \"Gini\": \"Gini\",\n",
        "    \"Top3Dominance\": \"Top-3 Proportion\",\n",
        "    \"AvgWordLength\": \"Mean Word Length\",\n",
        "    \"AvgTurnLength\": \"Mean Turn Length\",\n",
        "    \"Rel_MOT_Turns\": \"Conversation Share\",\n",
        "    \"Concreteness\": \"Concreteness\",\n",
        "}\n",
        "\n",
        "def pretty_feature_label(var):\n",
        "    if var.startswith(\"MOT_\"):\n",
        "        rest = var[4:]\n",
        "        for suffix in suffix_labels:\n",
        "            if rest.endswith(\"_\" + suffix):\n",
        "                base = rest[:-(len(suffix)+1)]\n",
        "                return var_labels.get(base, base)\n",
        "        return var_labels.get(rest, rest)\n",
        "    return var_labels.get(var, var)\n",
        "\n",
        "encoders = {}\n",
        "for col in [\"Language\", \"Study\", \"NumericID\", \"Gender\"]:\n",
        "    encoder = LabelEncoder()\n",
        "    df[col + \"_encoded\"] = encoder.fit_transform(df[col].astype(str))\n",
        "    encoders[col] = encoder\n",
        "\n",
        "\n",
        "\n",
        "class MultitaskTTRModel(nn.Module):\n",
        "    def __init__(self, n_langs, n_studies, n_children, n_genders, input_dim, embed_dim=8):\n",
        "        super().__init__()\n",
        "        self.lang_embed = nn.Embedding(n_langs, embed_dim)\n",
        "        self.study_embed = nn.Embedding(n_studies, embed_dim)\n",
        "        self.child_embed = nn.Embedding(n_children, embed_dim)\n",
        "        self.gender_embed = nn.Embedding(n_genders, embed_dim)\n",
        "\n",
        "        self.shared = nn.Sequential(\n",
        "            nn.Linear(input_dim + 4 * embed_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.ttr = nn.Sequential(nn.Linear(32, 1), nn.Sigmoid())\n",
        "        self.cv = nn.Sequential(nn.Linear(32, 1), nn.ReLU())\n",
        "        self.gini = nn.Sequential(nn.Linear(32, 1), nn.Sigmoid())\n",
        "        self.dom = nn.Sequential(nn.Linear(32, 1), nn.Sigmoid())\n",
        "\n",
        "    def forward(self, features, language, study, child, gender):\n",
        "        x = torch.cat([\n",
        "            features,\n",
        "            self.lang_embed(language),\n",
        "            self.study_embed(study),\n",
        "            self.child_embed(child),\n",
        "            self.gender_embed(gender)\n",
        "        ], dim=1)\n",
        "        x = self.shared(x)\n",
        "        return torch.cat([self.ttr(x), self.cv(x), self.gini(x), self.dom(x)], dim=1)\n",
        "\n",
        "target_labels = [\"TTR\", \"CV\", \"Gini\", \"Top3Dominance\"]\n",
        "\n",
        "\n",
        "\n",
        "output_types = [\"Content\", \"Matched\"]\n",
        "suffixes = [\n",
        "    \"EmbWord_t0.75\", \"EmbWord_t0.50\", \"EmbWord_t0.25\",\n",
        "    \"Syn\", \"Lex\",\n",
        "    \"ContentLemmaPOS\", \"ContentLemma\", \"Content\",\n",
        "    \"Raw\"\n",
        "]\n",
        "\n",
        "for suffix in suffixes:\n",
        "    for output_type in output_types:\n",
        "        output_suffix = suffix if output_type == \"Matched\" else \"Content\"\n",
        "        model_path = f\"best_model_ROTATED_{suffix}_TO_{output_suffix}.pt\"\n",
        "        print(f\"\\n Plotting: MOT_{suffix} ‚Üí CHI_{output_suffix}\")\n",
        "\n",
        "        feature_cols = [\n",
        "            f\"MOT_TTR_{suffix}\", f\"MOT_CV_{suffix}\", f\"MOT_Gini_{suffix}\", f\"MOT_Top3Dominance_{suffix}\",\n",
        "            \"MOT_AvgWordLength\", \"Rel_MOT_Turns\", \"MOT_AvgTurnLength\",\n",
        "            \"MOT_Concreteness\", \"Age_in_Days\"\n",
        "        ]\n",
        "        target_cols = [f\"CHI_{metric}_{output_suffix}\" for metric in [\"TTR\", \"CV\", \"Gini\", \"Top3Dominance\"]]\n",
        "\n",
        "        df_clean = df.dropna(subset=feature_cols + target_cols).copy()\n",
        "        if df_clean.empty:\n",
        "            print(f\" Skipping {suffix} ‚Üí {output_suffix} ‚Äî no data.\")\n",
        "            continue\n",
        "\n",
        "        top3_col = f\"CHI_Top3Dominance_{output_suffix}\"\n",
        "        top3_scaler = StandardScaler()\n",
        "        top3_scaler.fit(df_clean[[top3_col]])\n",
        "\n",
        "\n",
        "        model = MultitaskTTRModel(\n",
        "            n_langs=len(encoders[\"Language\"].classes_),\n",
        "            n_studies=len(encoders[\"Study\"].classes_),\n",
        "            n_children=len(encoders[\"NumericID\"].classes_),\n",
        "            n_genders=len(encoders[\"Gender\"].classes_),\n",
        "            input_dim=len(feature_cols)\n",
        "        ).to(device)\n",
        "\n",
        "        try:\n",
        "            model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "        except FileNotFoundError:\n",
        "            print(f\"‚ùå Missing model: {model_path}\")\n",
        "            continue\n",
        "\n",
        "        model.eval()\n",
        "        all_preds = []\n",
        "        mot_inputs = [f for f in feature_cols if f != \"Age_in_Days\"]\n",
        "        age_vals = np.linspace(df_clean[\"Age_in_Days\"].min(), df_clean[\"Age_in_Days\"].max(), 50)\n",
        "\n",
        "        for mot_var in mot_inputs:\n",
        "            other_vars = [f for f in feature_cols if f not in [\"Age_in_Days\", mot_var]]\n",
        "            cat_vars = [\"Language_encoded\", \"Study_encoded\", \"NumericID_encoded\", \"Gender_encoded\"]\n",
        "            samples = df_clean[other_vars + cat_vars].copy().reset_index(drop=True)\n",
        "            var_vals = np.linspace(df_clean[mot_var].min(), df_clean[mot_var].max(), 50)\n",
        "\n",
        "            preds_all = []\n",
        "            scaler = StandardScaler()\n",
        "            scaler.fit(df_clean[feature_cols])\n",
        "\n",
        "            for age in age_vals:\n",
        "                for mot_val in var_vals:\n",
        "                    temp = samples.copy()\n",
        "                    temp[\"Age_in_Days\"] = age\n",
        "                    temp[mot_var] = mot_val\n",
        "                    X = scaler.transform(temp[feature_cols])\n",
        "                    X_tensor = torch.tensor(X, dtype=torch.float32, device=device)\n",
        "\n",
        "                    lang = torch.tensor(samples[\"Language_encoded\"].values, device=device)\n",
        "                    study = torch.tensor(samples[\"Study_encoded\"].values, device=device)\n",
        "                    child = torch.tensor(samples[\"NumericID_encoded\"].values, device=device)\n",
        "                    gender = torch.tensor(samples[\"Gender_encoded\"].values, device=device)\n",
        "\n",
        "\n",
        "                    with torch.no_grad():\n",
        "                        pred = model(X_tensor, lang, study, child, gender).cpu().numpy()\n",
        "\n",
        "\n",
        "                    preds_all.append(pred.mean(axis=0))\n",
        "\n",
        "            avg_preds = np.array(preds_all).reshape(len(var_vals), len(age_vals), 4)\n",
        "            all_preds.append(avg_preds)\n",
        "\n",
        "        fig, axes = plt.subplots(len(mot_inputs), 4, figsize=(4 * 4, len(mot_inputs) * 3))\n",
        "        Zmins = [np.inf] * 4\n",
        "        Zmaxs = [-np.inf] * 4\n",
        "        for j in range(4):\n",
        "            for pred in all_preds:\n",
        "                Zmins[j] = min(Zmins[j], pred[:, :, j].min())\n",
        "                Zmaxs[j] = max(Zmaxs[j], pred[:, :, j].max())\n",
        "\n",
        "        for i, mot_var in enumerate(mot_inputs):\n",
        "            var_vals = np.linspace(df_clean[mot_var].min(), df_clean[mot_var].max(), 50)\n",
        "            avg_preds = all_preds[i]\n",
        "            shared_ylim = (var_vals.min(), var_vals.max())\n",
        "            shared_locator = MaxNLocator(nbins=3)\n",
        "            shared_formatter = FormatStrFormatter('%.2f')\n",
        "\n",
        "            for j, label in enumerate([\"TTR\", \"CV\", \"Gini\", \"Top3Dominance\"]):\n",
        "                ax = axes[i, j] if len(mot_inputs) > 1 else axes[j]\n",
        "                Z = avg_preds[:, :, j]\n",
        "                contour = ax.contourf(age_vals / 365.25, var_vals, Z, levels=50, cmap=\"viridis\",\n",
        "                                      vmin=Zmins[j], vmax=Zmaxs[j])\n",
        "\n",
        "                ax.set_ylim(shared_ylim)\n",
        "                ax.yaxis.set_major_locator(shared_locator)\n",
        "                ax.yaxis.set_major_formatter(shared_formatter)\n",
        "\n",
        "\n",
        "                if i == 0:\n",
        "                    ax.set_title(var_labels.get(label, label), fontsize=14)\n",
        "                if j == 0:\n",
        "                    ax.set_ylabel(pretty_feature_label(mot_var), fontsize=14)\n",
        "                    ax.yaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
        "                    ax.yaxis.set_major_locator(MaxNLocator(nbins=3))\n",
        "\n",
        "                ticks = np.arange(1, 6, 1)\n",
        "                ax.set_xticks(ticks)\n",
        "                if i == len(mot_inputs) - 1:\n",
        "                    ax.set_xticklabels([str(int(t)) for t in ticks])\n",
        "                else:\n",
        "                    ax.set_xticklabels([])\n",
        "\n",
        "                for xtick in ticks:\n",
        "                    ax.axvline(x=xtick, color=\"white\", linestyle=\"--\", linewidth=0.5, alpha=0.3)\n",
        "\n",
        "\n",
        "                if j != 0:\n",
        "                    ax.set_yticklabels([])\n",
        "                ax.set_xlabel(\"\")\n",
        "\n",
        "                ax.tick_params(labelsize=12)\n",
        "                divider = make_axes_locatable(ax)\n",
        "                cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
        "                cbar = fig.colorbar(contour, cax=cax)\n",
        "                cbar.ax.tick_params(labelsize=12)\n",
        "                ticks = np.linspace(cbar.vmin, cbar.vmax, 3)\n",
        "                cbar.set_ticks(ticks)\n",
        "                cbar.ax.yaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
        "\n",
        "\n",
        "        title_suffix = \"‚Üí CHI_Content\" if output_type == \"Content\" else f\"‚Üí CHI_{suffix}\"\n",
        "\n",
        "        plt.tight_layout(rect=[0, 0.03, 0.95, 0.94])\n",
        "\n",
        "        plt.subplots_adjust(wspace=0.3, hspace=0.3)\n",
        "\n",
        "\n",
        "        fig.text(0.5, 0.94, f\"Child ({suffix_labels.get(output_suffix, output_suffix)})\",\n",
        "         ha=\"center\", va=\"bottom\", fontsize=20)\n",
        "\n",
        "        fig.text(-0.04, 0.5, f\"Mother ({suffix_labels.get(suffix, suffix)})\",\n",
        "                ha=\"center\", va=\"center\", rotation=\"vertical\", fontsize=20)\n",
        "\n",
        "        fig.text(0.5, 0.02, \"Age (years)\", ha=\"center\", va=\"center\", fontsize=16)\n",
        "\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "\n",
        "        del all_preds\n",
        "        del model\n",
        "        gc.collect()\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "id": "VFRgDUyh1t7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "import torch.nn as nn\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "from matplotlib.ticker import FormatStrFormatter, MaxNLocator\n",
        "import gc\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "df = pd.read_excel(\"/content/childes_data_filtered_2.xlsx\")\n",
        "df[\"Age_in_Days\"] = df[\"Age_in_Days\"].astype(float)\n",
        "\n",
        "\n",
        "\n",
        "pred_indices = {\"TTR\": 0, \"CV\": 1, \"Gini\": 2, \"Top3Dominance\": 3}\n",
        "\n",
        "\n",
        "suffix_labels = {\n",
        "    \"Raw\": \"All Words\",\n",
        "    \"Content\": \"Content Words\",\n",
        "    \"ContentLemma\": \"Lemmatized Content Words\",\n",
        "    \"ContentLemmaPOS\": \"Lemmatized Content Words with POS\",\n",
        "    \"Syn\": \"Synonyms\",\n",
        "    \"Lex\": \"Lexical Category\",\n",
        "    \"EmbWord_t0.25\": \"Word Embeddings  T = 0.25\",\n",
        "    \"EmbWord_t0.50\": \"Word Embeddings T = 0.5\",\n",
        "    \"EmbWord_t0.75\": \"Word Embeddings T = 0.75\",\n",
        "}\n",
        "\n",
        "var_labels = {\n",
        "    \"TTR\": \"TTR\",\n",
        "    \"CV\": \"CV\",\n",
        "    \"Gini\": \"Gini\",\n",
        "    \"Top3Dominance\": \"Top-3 Proportion\",\n",
        "    \"AvgWordLength\": \"Mean Word Length\",\n",
        "    \"AvgTurnLength\": \"Mean Turn Length\",\n",
        "    \"Rel_MOT_Turns\": \"Conversation Share\",\n",
        "    \"Concreteness\": \"Concreteness\",\n",
        "}\n",
        "\n",
        "\n",
        "def pretty_feature_label(var):\n",
        "    if var.startswith(\"MOT_\"):\n",
        "        rest = var[4:]\n",
        "        for suffix in suffix_labels:\n",
        "            if rest.endswith(\"_\" + suffix):\n",
        "                base = rest[:-(len(suffix)+1)]\n",
        "                return var_labels.get(base, base)\n",
        "        return var_labels.get(rest, rest)\n",
        "    return var_labels.get(var, var)\n",
        "\n",
        "encoders = {}\n",
        "for col in [\"Language\", \"Study\", \"NumericID\", \"Gender\"]:\n",
        "    encoder = LabelEncoder()\n",
        "    df[col + \"_encoded\"] = encoder.fit_transform(df[col].astype(str))\n",
        "    encoders[col] = encoder\n",
        "\n",
        "\n",
        "\n",
        "class MultitaskTTRModel(nn.Module):\n",
        "    def __init__(self, n_langs, n_studies, n_children, n_genders, input_dim, embed_dim=8):\n",
        "        super().__init__()\n",
        "        self.lang_embed = nn.Embedding(n_langs, embed_dim)\n",
        "        self.study_embed = nn.Embedding(n_studies, embed_dim)\n",
        "        self.child_embed = nn.Embedding(n_children, embed_dim)\n",
        "        self.gender_embed = nn.Embedding(n_genders, embed_dim)\n",
        "\n",
        "        self.shared = nn.Sequential(\n",
        "            nn.Linear(input_dim + 4 * embed_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.ttr = nn.Sequential(nn.Linear(32, 1), nn.Sigmoid())\n",
        "        self.cv = nn.Sequential(nn.Linear(32, 1), nn.ReLU())\n",
        "        self.gini = nn.Sequential(nn.Linear(32, 1), nn.Sigmoid())\n",
        "        self.dom = nn.Sequential(nn.Linear(32, 1), nn.Sigmoid())\n",
        "\n",
        "    def forward(self, features, language, study, child, gender):\n",
        "        x = torch.cat([\n",
        "            features,\n",
        "            self.lang_embed(language),\n",
        "            self.study_embed(study),\n",
        "            self.child_embed(child),\n",
        "            self.gender_embed(gender)\n",
        "        ], dim=1)\n",
        "        x = self.shared(x)\n",
        "        return torch.cat([self.ttr(x), self.cv(x), self.gini(x), self.dom(x)], dim=1)\n",
        "\n",
        "\n",
        "\n",
        "output_types = [\"Content\", \"Matched\"]\n",
        "suffixes = [\n",
        "    \"EmbWord_t0.75\", \"EmbWord_t0.50\", \"EmbWord_t0.25\",\n",
        "    \"Syn\", \"Lex\",\n",
        "    \"ContentLemmaPOS\", \"ContentLemma\", \"Content\",\n",
        "    \"Raw\"\n",
        "]\n",
        "\n",
        "for suffix in suffixes:\n",
        "\n",
        "    if suffix == \"Lex\":\n",
        "        target_labels = [\"TTR\", \"CV\"]\n",
        "    else:\n",
        "        target_labels = [\"TTR\", \"Gini\"]\n",
        "\n",
        "    for output_type in output_types:\n",
        "        output_suffix = suffix if output_type == \"Matched\" else \"Content\"\n",
        "        model_path = f\"best_model_ROTATED_{suffix}_TO_{output_suffix}.pt\"\n",
        "        print(f\"\\n Plotting: MOT_{suffix} ‚Üí CHI_{output_suffix}\")\n",
        "\n",
        "        feature_cols = [\n",
        "            f\"MOT_TTR_{suffix}\", f\"MOT_CV_{suffix}\", f\"MOT_Gini_{suffix}\", f\"MOT_Top3Dominance_{suffix}\",\n",
        "            \"MOT_AvgWordLength\", \"Rel_MOT_Turns\", \"MOT_AvgTurnLength\",\n",
        "            \"MOT_Concreteness\", \"Age_in_Days\"\n",
        "        ]\n",
        "        target_cols = [f\"CHI_{metric}_{output_suffix}\" for metric in [\"TTR\", \"CV\", \"Gini\", \"Top3Dominance\"]]\n",
        "\n",
        "        df_clean = df.dropna(subset=feature_cols + target_cols).copy()\n",
        "        if df_clean.empty:\n",
        "            print(f\" Skipping {suffix} ‚Üí {output_suffix} ‚Äî no data.\")\n",
        "            continue\n",
        "\n",
        "        top3_col = f\"CHI_Top3Dominance_{output_suffix}\"\n",
        "        top3_scaler = StandardScaler()\n",
        "        top3_scaler.fit(df_clean[[top3_col]])\n",
        "\n",
        "\n",
        "        model = MultitaskTTRModel(\n",
        "            n_langs=len(encoders[\"Language\"].classes_),\n",
        "            n_studies=len(encoders[\"Study\"].classes_),\n",
        "            n_children=len(encoders[\"NumericID\"].classes_),\n",
        "            n_genders=len(encoders[\"Gender\"].classes_),\n",
        "            input_dim=len(feature_cols)\n",
        "        ).to(device)\n",
        "\n",
        "        try:\n",
        "            model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "        except FileNotFoundError:\n",
        "            print(f\"‚ùå Missing model: {model_path}\")\n",
        "            continue\n",
        "\n",
        "        model.eval()\n",
        "        all_preds = []\n",
        "        allowed_inputs = [\"MOT_TTR_\", \"MOT_CV_\", \"MOT_Gini_\", \"MOT_Top3Dominance_\"]\n",
        "\n",
        "        mot_inputs = [\n",
        "            f for f in feature_cols\n",
        "            if f != \"Age_in_Days\" and any(f.startswith(prefix) for prefix in allowed_inputs)\n",
        "        ]\n",
        "\n",
        "        row_order = [\"TTR\", \"CV\", \"Gini\", \"Top3Dominance\"]\n",
        "\n",
        "        mot_inputs = sorted(mot_inputs, key=lambda x: row_order.index(x.split(\"_\")[1]))\n",
        "\n",
        "        age_vals = np.linspace(df_clean[\"Age_in_Days\"].min(), df_clean[\"Age_in_Days\"].max(), 50)\n",
        "\n",
        "        for mot_var in mot_inputs:\n",
        "            other_vars = [f for f in feature_cols if f not in [\"Age_in_Days\", mot_var]]\n",
        "            cat_vars = [\"Language_encoded\", \"Study_encoded\", \"NumericID_encoded\", \"Gender_encoded\"]\n",
        "            samples = df_clean[other_vars + cat_vars].copy().reset_index(drop=True)\n",
        "            var_vals = np.linspace(df_clean[mot_var].min(), df_clean[mot_var].max(), 50)\n",
        "\n",
        "            preds_all = []\n",
        "            scaler = StandardScaler()\n",
        "            scaler.fit(df_clean[feature_cols])\n",
        "\n",
        "            for age in age_vals:\n",
        "                for mot_val in var_vals:\n",
        "                    temp = samples.copy()\n",
        "                    temp[\"Age_in_Days\"] = age\n",
        "                    temp[mot_var] = mot_val\n",
        "                    X = scaler.transform(temp[feature_cols])\n",
        "                    X_tensor = torch.tensor(X, dtype=torch.float32, device=device)\n",
        "\n",
        "                    lang = torch.tensor(samples[\"Language_encoded\"].values, device=device)\n",
        "                    study = torch.tensor(samples[\"Study_encoded\"].values, device=device)\n",
        "                    child = torch.tensor(samples[\"NumericID_encoded\"].values, device=device)\n",
        "                    gender = torch.tensor(samples[\"Gender_encoded\"].values, device=device)\n",
        "\n",
        "\n",
        "                    with torch.no_grad():\n",
        "                        pred = model(X_tensor, lang, study, child, gender).cpu().numpy()\n",
        "\n",
        "                    preds_all.append(pred.mean(axis=0))\n",
        "\n",
        "            avg_preds = np.array(preds_all).reshape(len(var_vals), len(age_vals), 4)\n",
        "            all_preds.append(avg_preds)\n",
        "\n",
        "        fig, axes = plt.subplots(len(mot_inputs), len(target_labels), figsize=(16, 24))\n",
        "\n",
        "\n",
        "        Zmins = {}\n",
        "        Zmaxs = {}\n",
        "        for label in target_labels:\n",
        "            idx = pred_indices[label]\n",
        "            Zmins[label] = min(pred[:, :, idx].min() for pred in all_preds)\n",
        "            Zmaxs[label] = max(pred[:, :, idx].max() for pred in all_preds)\n",
        "\n",
        "\n",
        "        for i, mot_var in enumerate(mot_inputs):\n",
        "            var_vals = np.linspace(df_clean[mot_var].min(), df_clean[mot_var].max(), 50)\n",
        "            avg_preds = all_preds[i]\n",
        "            shared_ylim = (var_vals.min(), var_vals.max())\n",
        "            shared_locator = MaxNLocator(nbins=3)\n",
        "            shared_formatter = FormatStrFormatter('%.2f')\n",
        "\n",
        "            for j, label in enumerate(target_labels):\n",
        "\n",
        "                ax = axes[i, j] if len(mot_inputs) > 1 else axes[j]\n",
        "                Z = avg_preds[:, :, pred_indices[label]]\n",
        "\n",
        "                contour = ax.contourf(age_vals / 365.25, var_vals, Z, levels=50, cmap=\"viridis\",\n",
        "                                      vmin = Zmins[label], vmax = Zmaxs[label])\n",
        "\n",
        "                ax.set_ylim(shared_ylim)\n",
        "                ax.yaxis.set_major_locator(shared_locator)\n",
        "                ax.yaxis.set_major_formatter(shared_formatter)\n",
        "\n",
        "\n",
        "                if i == 0:\n",
        "                    ax.set_title(var_labels.get(label, label), fontsize=22)\n",
        "                if j == 0:\n",
        "                    ax.set_ylabel(pretty_feature_label(mot_var), fontsize=22)\n",
        "                    ax.yaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
        "                    ax.yaxis.set_major_locator(MaxNLocator(nbins=3))\n",
        "\n",
        "                ticks = np.arange(1, 6, 1)\n",
        "                ax.set_xticks(ticks)\n",
        "                if i == len(mot_inputs) - 1:\n",
        "                    ax.set_xticklabels([str(int(t)) for t in ticks])\n",
        "                else:\n",
        "                    ax.set_xticklabels([])\n",
        "\n",
        "                for xtick in ticks:\n",
        "                    ax.axvline(x=xtick, color=\"white\", linestyle=\"--\", linewidth=1.5, alpha=0.3)\n",
        "\n",
        "\n",
        "                if j != 0:\n",
        "                    ax.set_yticklabels([])\n",
        "                ax.set_xlabel(\"\")\n",
        "\n",
        "                ax.tick_params(labelsize=18, length=8)\n",
        "                divider = make_axes_locatable(ax)\n",
        "                cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
        "                cbar = fig.colorbar(contour, cax=cax)\n",
        "                cbar.ax.tick_params(labelsize=18)\n",
        "                ticks = np.linspace(cbar.vmin, cbar.vmax, 3)\n",
        "                cbar.set_ticks(ticks)\n",
        "                cbar.ax.yaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
        "\n",
        "\n",
        "        title_suffix = \"‚Üí CHI_Content\" if output_type == \"Content\" else f\"‚Üí CHI_{suffix}\"\n",
        "\n",
        "        plt.tight_layout(rect=[0, 0.03, 0.95, 0.94])\n",
        "\n",
        "        plt.subplots_adjust(wspace=0.3, hspace=0.3)\n",
        "\n",
        "\n",
        "        fig.text(0.5, 0.94, f\"Child ({suffix_labels.get(output_suffix, output_suffix)})\",\n",
        "         ha=\"center\", va=\"bottom\", fontsize=24)\n",
        "\n",
        "        fig.text(-0.04, 0.5, f\"Mother ({suffix_labels.get(suffix, suffix)})\",\n",
        "                ha=\"center\", va=\"center\", rotation=\"vertical\", fontsize=24)\n",
        "\n",
        "        fig.text(0.5, 0.02, \"Age (years)\", ha=\"center\", va=\"center\", fontsize=20)\n",
        "\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "\n",
        "        del all_preds\n",
        "        del model\n",
        "        gc.collect()\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "id": "dwBAu9511v0a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}